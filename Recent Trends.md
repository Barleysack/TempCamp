# Recent Trends

왼쪽부터 시작해서 하나하나 단어를 생성한다는 '그리디 디코딩' 에서 벗어나지는 못하고 있습니다.

### GPT-1

------

특징: 다양한 스페셜 토큰을 제안하여, 심플한 태스크 뿐 아니라 다양한 태스크를 동시에 커버할 수 있는 통합된 모델을 제안하였다.  셀프 어텐션 블록을 열두개 쌓은 그런 구조인데,

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e45ccdbe-5c1d-42f6-8eaa-2b3b9ddb38f9/Untitled.png)

특수한 토큰을 넣어 간단한 단어 추론 이외에도 여러 태스크를 가능케 한 듯 하다. 저 Extract 토큰이 이제 아웃풋으로 최종적으로 나온다는 듯 하다.

extract에 해당하는 그 벡터를 최종적인 아웃풋 레이어에 통과시켜 논리적 분류 태스크를 수행하게 되는 듯.  extract 토큰이 일종의 쿼리로 사용이 되어서 주어진 입력 문장으로부터 정보를 적절하게 추출할 수 있습니다.

- 이미 학습된 트랜스포머와, 이후 태스크를 위한 추가적 레이어 한개만을 더해 해당 태스크를 수행하는 것입니다. 상대적으로 러닝레이트를 작게 줌으로서, 기존에 사용한 정보를 최대한 변질시키지 않고 사용하려는 아이디어인 듯 합니다.
- 별도의 레이블이 필요하지 않은 스텝에서 셀프-수퍼바이즈드 러닝 된 지식을 소량의 데이터셋만이 있는 분야를 해결해나가는 듯 합니다.

### BERT

------

### 

motivation: 전 후 문맥을 봐야하는데 앞 문장밖에 모르거나 뒤밖에 모른다면 유추하기가 쉽지 않지 않은가?

### MLM ( pre training tasks in bert)

몇몇 단어를 마스킹하여 트레이닝 하는데,

너무 작은 마스킹 : 너무 많은 비용이 필요한 훈련

너무 많은 마스킹 : 문맥을 파악하기 충분하지 않게 됨.

Bert에선 15퍼센트 정도를 마스킹한다고 한다.

15퍼센트 단어를 완전히 마스킹하지 않는 이유는, 마스크 토큰이 아예 없을 경우에는 다른 특성을 보일 수 있기 때문에, 0.8을 마스킹, 0.1을 랜덤단어로 치환하고, 0.1을 그대로 둔다고 한다.

### Next Sentence Prediction

------

문장간의 관계를 알기 위해, 두 문장을 두어 뒤의 문장이 앞 문장에 이어지는 문장인지, 아니면 그냥 랜덤 문장인지(연속적인 문장이 아닌지) 예측하는 태스크. 간단한 바이너리 클래시피케이션인듯.

### Arch

------

기본적으로 셀프-어텐션 블록을 그대로 가져와서 쌓은 모습입니다. 각 셀프 어텐션 블럭에서 더 많은 수의 헤드를 사용합니다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/57dcbcf8-7926-42b5-900e-661e75de11d8/Untitled.png)

A가 헤드수인듯, H는 각 셀프어텐션 블럭에서 유지하는 인코딩 벡터의 차원 수입니다.

보통은 입력으로 서브워드 임베딩을 받습니다만, 직관적으로 PRETRAIN이라는 단어가 있다면

pre, training 두가지로 나누어서 갈 것 입니다.

BERT에선 이 포지셔널 임베딩도 학습에 의해서 결정된다고 한다.

세그먼트 임베딩?

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/dc3f95a6-1771-41cd-a98d-2c71c47f9bdb/Untitled.png)

특정한 타임스텝에서는 자기자신을 포함해 왼쪽의 정보를 엑세스 하고 있는 것을 확인할 수 있습니다만, 그로 인해 GPT에서는 마스크드-셀프어텐션을 사용합니다만,  BERT에서는 트랜스포머 인코더에서 사용하던 그 모듈을 사용하는 듯 합니다. 모든 단어가 모든 단어를 바라볼 수 있는 것이라고 할 수 있습니다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2109c5fe-418e-465b-b2c4-301c780bf0e9/Untitled.png)

이들 모델들을 활용한 여러 태스크에 익숙해질 필요성이 있다.

### MRC, QA 등...

## CLS 토큰에 대해 알아볼래?