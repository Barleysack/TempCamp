# Seq-to-Seq Attention

---





seq to seq model : rnn중 many-many 형태에 해당합니다. 모든 입력을 받고, 출력을 받기 시작하는 형식입니다.

입력문장을 읽어내는 rnn 모듈을 Encoder, 출력문장을 단어 하나하나씩 뱉어주는 rnn모듈을 Decoder라고 부릅니다.  인코더와 디코더는 다른 파라미터를 사용합니다.

rnn 모델로서 lstm을 채용한 것을 볼 수 있습니다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3402e119-98df-4f9a-a55d-93dfa2bdd246/Untitled.png)

시작하는 단어를 넣어주는 것을 스타트토큰 <sos> 로 알아보게 됩니다.

문장이 끝나는 시점은 <end> 토큰으로 나타내게 됩니다 (EOS, END OF SENTENCE)

Attention 모듈의 기본적인 모티베이션은 seq2seq에서 순차별로 매 타임스텝마다 정보를  저장해가며 히든스테이트 벡터를 생성하는 과정을 따르는데, 이때 히든스테이트 벡터의 디멘전은 정해져있기에, 입력 문장의 길이에 상관 없이 마지막 타임스텝의 히든스테이트벡터에 모든 정보를 꾸겨넣게 된다.

마지막 히든에서는 모든 정보를 그 좁은 벡터에 우겨넣어야 한다.

- 이게 왜 안좋은가?

  ```
  lstm으로 해도 그래디언트 소실이 일어날 수 있습니다. 
  ```

정보는 소실되거나 변질될 수 있습니다. 기계번역을 예로 들어,  i go home을 번역해야 하는 경우, 주어를 먼저 번역해야하는데, 인코더의 마지막 타임스텝에서 가장 첫번째 단어의 정보가 소실되어 번역된 문장의 품질이 낮아질 수 있기 때문에, 아예 입력문장의 순서를 뒤집는 테크닉 또한 사용하였습니다.

### 어텐션 모듈?

디코더에서 인코더의 마지막 타임스텝만을 의존하는 것이 아니라, 인코더의 히든스테이트 벡터를 각각의 단어에서 나온 히든스테이트 벡터는 각각의 단어에 대응하는, 조금 더 관련이 있는 벡터일텐데, 이를 각각 디코더에 공급하며, 디코더는 상관관계가 있는 인코더의 히든스테이트를 가져가 활용하게 됩니다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/188dcc21-b520-4305-8a65-a36ac1bc79dc/Untitled.png)

인코더의 마지막 히든스테이트가 디코더의 0번째 히든스테이트로서 들어가고, 디코더에는 start 토큰(x1, 워드 임베딩 벡터)이 공급되었을때, 첫번째 히든스테이트가 만들어져 다음 단어를 예측  뿐 아니라 인코더의 히든 스테이트 벡터중 현재 어떤 히든스테이트 벡터를 가져와야하는지 선정하는 역할 또한 하게 됩니다. 각각의 인코더 히든스테이트 벡터와 내적을 해, 소프트맥스를 통과시켜 로짓값으로 가져오게 됩니다.  결국 그 로짓값은 각각의 인코더 히든스테이트벡터의 가중치로서 사용되게 되고, 이는 가중평균으로 사용되게 되고 , 그로서 나오는 하나의 인코딩 벡터를 구하게 됩니다. 인코더 가중평균 벡터는 이제 컨텍스트 벡터라고 부릅니다. 어텐션 모듈의 출력은 가중평균인 한 벡터만이 나오게 되는 것입니다. 앞서 계산한 디코더 히든스테이트 벡터 + 어텐션 아웃풋 컨텍스트 벡터 가 concat 되어 다음 나올 단어를 예측하게 된다.

마찬가지로, 첫번째 단어를 예측한 이후, 디코더에서의 두번째 타임스텝에서도 동일한 어텐션 모듈을 사용하며, 역시 유사도를 계산하고, 소프트맥스를 통과시켜 얻은 가중치에 따른 컨텍스트 벡터를 얻어내고, 이를 다시 두번째 히든스테이트 벡터와 컨텍스트 벡터를 컨캣시켜 두번째 단어를 예측하게 됩니다.

- 즉, 디코더 아웃풋인 디코더 히든스테이트는 다음 단어를 예측하는 역할과, 인코더 히든스테이트를 취사선택하는 두 역할을 동시에 하게 됩니다.
- 백프롭의 과정에서도, 디코더의 방향으로 백프롭이 일어나기도 하지만, 동시에 어텐션 모듈 방향에서도 백프롭이 일어납니다.  인코더 단에서 정보를 잘못 가져왔을 경우에, 저 가중치를 적절히 조정하며 적절한 정보를 가져올 수 있도록 학습하게 됩니다.

각 타임스텝마다 올바르지 못한 단어를 예측했다 하더라도 학습동안은 GT로서의 올바른 단어를 입력해주게 됩니다만, 추론에서는 잘못 하게 되었더라도 그를 다시 GT로 넣게 됩니다. 그렇군요. 학습때는 어쨌든 GT를 써먹게 되는군요.

이를 Teacher Forcing이라고 부릅니다.

Teacher forcing이 아닌 방식이 조금 더 실생활과 가까운 방식이라고 할 수 있습니다. 적절히 결합하는 것이 포인트라는듯.

이 유사도를 구하는 과정을 다양하게 변화하는 메커니즘이 많습니다!

### Attention mechanisms...!

이건 직접 필기했다간 울 가능성이 많다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/3c838b1b-7597-4409-ac43-6b9c18776383/Untitled.png)

attention score를 구하는 방식 중 세가지 정도를 알려주시는데,

그냥 내적을 하기도 하고, 제너럴-연산도 사용한다고 하는데...

이는 무슨 말씀일까?

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f9f9cefd-9ef9-472a-9a34-5d2d2f28363d/Untitled.png)

세상에... 가운데에 학습 가능한 행렬을 추가함으로서 ,

이 내적이라는 단순한 연산을 확장할 수 있는 것이다.

더 일반화된 내적(general) 로 구성할 수 있는 것 같다.

- Concat은?

유사도를 구하는 일종의 작은 뉴럴 네트워크를 만들어낼 수 있습니다.

여기서 조금 더 레이어를 쌓아서,

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f90fd62c-ba7b-4e65-837d-b1343ee55dbb/Untitled.png)

첫번째 레이어의 선형변환을 거치고, Wa 와 Va를 학습하게 되는 듯 합니다. W2가 이제 결국은 스칼라값을 뱉어내는 방향으로 만들어지게 됩니다.

이렇게도 표현이 가능합니다.

이는 저 Attention score 구하는 심플한 내적의 연산 파트가 이제 Trainable 한 파라미터가 포함이 된 작은 모듈로 변하는 것이라고 할 수 있습니다. 추가적인 학습가능한 파라미터를 추가하는 것이라고 할 수 있습니다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/66b4a940-b49d-40c3-b08c-492b0a4c20bf/Untitled.png)

성능을 한참 올렸고, 긴 문장에서 번역이 어렵던 것도 해결했으며, 그래디언트 소실 문제 또한 원래라면 많은 타임스텝을 거쳐야 해서 먼 타임스텝간의 정보 보존이 어려웠겠지만, 어텐션을 사용하며 정보의 일종의 지름길이 생성되게 되었습니다. 어떠한 타임스텝도 거치지 않았다는 것입니다. 큰 변질 없이 전달이 되었다는 것이지요.

해석 가능성도 주는데, 이 어텐션의 패턴을 조사함으로서, 인코더 상의 어떤 단어에 집중했는지 확인할 수 있게 됩니다. 언제, 어떤 단어를 보아야 할지 고르는 얼라인먼트를 알아서 배우게 되는 것입니다.

## Beam Search

------

------

바로 다음 단어를 예측하는 과정을 학습하고, 테스트 타임 동안 매 타임스텝마다 가장 높은 확률을 가지는 단어 하나를 가지고 디코딩을 진행합니다. 이를 `그리디 디코딩` 이라고 부릅니다. 근시안 적으로 그때그때 가장 좋아보이는 것을 접근하는 것. 따라서 한개 잘못 추론하면 쉽지 않아집니다. 이를 해결하기 위해서는 무엇을 해야할까요?

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d75c8aa0-0645-41bd-888f-eedc1683fd13/Untitled.png)

앞에서 가장 큰 확률값을 선택하는게 아니라 적당한걸 선택함으로서 뒤의 확률에서 뒤집힐 수 있도록 하는게 더 훌륭한 것인 듯 합니다.다만, 모든 경우의 수를 타임스텝마다 확인하는건 시간복잡도상 말도 안되기 때문에, 빔 서치를 고려합니다.

차선책, `빔서치`

매 타임스텝마다 하나의 단어만을 고려하는 것도, 모두를 고려하는 것도 아니고, k개의 경우의 수를 정해 그만큼의 캔디데이트중 확률이 가장 높은 것을 택하는 것입니다. 디코더의 하나하나 아웃풋을 가설값으로 가지며, k는 보통 5~ 10 입니다. 로그를 붙여 합연산으로 변환해서 계산합니다.

빔 서치가 반드시 쩌는건 아니지만, 그래도 일반적 성능이 더 훌륭합니다.

빔 사이즈를 2라고 생각할때, 저 해당 로그 prob을 계산해내는 것입니다. 보면 알 수 있으리라 생각합니다. 화이팅이십니다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e4012c1b-ec9e-40df-baa6-97b350b4eaab/Untitled.png)

가장 확률이 큰 ( 절대값 아닌) 방향으로 쭉쭉 진행하게 됩니다. 계산 당시에는 그래서 k개만의 경우의 수만 고려하면 된다구요 . 어느 hypothesis가 end를 만나면, 그 길은 이제 고려하지 않고 따로 저장해둡니다.

빔-서치는 t라는 타임스텝의 최대값이 있을때 거기까지만 디코딩하고 저장하는 것입니다.

아니면 정해둔 n개만큼의 가설이 모이면 그만두게 됩니다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/cedefd29-4251-46b9-969e-e18ab9542386/Untitled.png)

이때 위와 같은 문제가 생기고(당연히 길어지면 계속 빼대니 작아질 수 밖에...)

## BLEU Score?

```
precision and recall
```

accuracy은 위치에 맞게 그냥 확인하는 것 뿐이라, 이것 만으로는 평가가 쉽지 않은 듯 하다.

precision은 그저 위치에 상관 없이 gt와 맞는 단어를 확인하고 prediction의 길이에 나눕니다.

recall 은 이 역시 gt와 겹치는 단어를 확인하지만, 이제 그것을 reference의 길이에 나눕니다.

f-score는 precision과 recall의 조화평균을 나타냅니다.

다만 얘네는 순서가 틀린건 확인할 수 없게 됩니다. 이것 또한 문제로서 받아들여질 수 있습니다.

이를 위해 사용 하는 것이 BLEU 스코어입니다.

각각의 단어로 겹치는걸 확인하는게 아니라, n그램이라고 하는 길이가 다른 단어뭉치 통으로 GT와 얼마나 겹치는걸 확인하는 방식이 있습니다.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/a0571ded-7aaf-445d-a4b6-a996295fafa6/Untitled.png)

이번의 길이가 전체의 길이보다 짧다면, 그 짧은 만큼 뒤의 정확도 합 값을 낮춰주겠다.

이를 brevity penalty 라고 부르는 듯 합니다.